{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nsclc-radiomics_demo.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO66wau9SjDSrYR8AUOBf+R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/IDC-Examples/blob/master/notebooks/nsclc-radiomics/nsclc_radiomics_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN1rTs8WUFHa"
      },
      "source": [
        "# <center> Rationale </center> \n",
        "\n",
        "This notebook stores a demo for Hosny et Al. [Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002711), reproduced using the tools provided by the Imaging Data Commons.\n",
        "\n",
        "<br>\n",
        "\n",
        "The goal of this notebook is to provide the user with an example of how the tools provided by the Imaging Data Commons could be used to run an AI/ML end-to-end analysis on a cohort hosted by the portal, and to describe what we identified as the best practices to do so.\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQn3IsWUVZ59"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# <center> Environment Setup, Data Download and Pre-processing </center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3jubIO2owdo"
      },
      "source": [
        "curr_dir = !pwd\n",
        "curr_droid = !hostname\n",
        "curr_pilot = !whoami\n",
        "\n",
        "print(\"Current directory :\", curr_dir[-1])\n",
        "print(\"Hostname          :\", curr_droid[-1])\n",
        "print(\"Username          :\", curr_pilot[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrJdKHTeaCq9"
      },
      "source": [
        "## Environment Set Up\n",
        "\n",
        "This demo notebook was conceived to be run using a GPU.\n",
        "\n",
        "To access a free GPU on Colab:\n",
        "`Edit > Notebooks Settings`\n",
        "\n",
        "From the dropdown menu under `Hardware accelerator`, select `GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mX8Lo5k6Ejo"
      },
      "source": [
        "# check wether the use of a GPU was correctly enabled\n",
        "gpu_list = !nvidia-smi --list-gpus\n",
        "\n",
        "has_gpu = False if \"failed\" in gpu_list[0] else True\n",
        "\n",
        "has_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bndO5Y7sGsoQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Preliminary Notes\n",
        "\n",
        "Hosny et Al. model was developed using Keras 1.2.2 and an old version of Tensorflow, as stated by the authors (e.g., see [the docker config file in the model GitHub repository](https://github.com/modelhub-ai/deep-prognosis/blob/master/dockerfiles/keras:1.0.1)). Since Google Colab instances are running either TensorFlow 2.x.x or TensorFlow 1.15.2, and Keras 2.x.x, pulling the model from the [project repository](https://github.com/modelhub-ai/deep-prognosis) will not work out-of-the-box (due to compatibility issues between Keras 1.x.x and Keras 2.x.x). \n",
        "\n",
        "<br>\n",
        "\n",
        "While it is possible to [use the `%tensorflow_version 1.x` magic to switch to the latest 1.x version of TensorFlow](https://colab.research.google.com/notebooks/tensorflow_version.ipynb)<sup>*</sup>, Colab does not allow to switch to older version of Keras. Fortunately, the solution to this issue is known and discussed in [various threads](https://github.com/keras-team/keras/issues/6382#issuecomment-530258501). Together with this notebook we hence provide a [Keras-2-compatible network configuration JSON file](https://github.com/ImagingDataCommons/IDC-Examples/blob/master/notebooks/nsclc-radiomics/demo/architecture.json).\n",
        "\n",
        "<sup>*</sup> the magic command must be run before importing TensorFlow.\n",
        "\n",
        "<br>\n",
        "\n",
        "Such JSON file is stored, together with the source code and the files needed for this demo, under the [Imagin Data Common's \"IDC-Examples\" repository](https://github.com/ImagingDataCommons/IDC-Examples) (specifically, under [IDC-Examples/notebooks/nsclc-radiomics/](https://github.com/ImagingDataCommons/IDC-Examples/tree/master/notebooks/nsclc-radiomics). All the other \n",
        "\n",
        "Since we are interested in cloning only a subdirectory of the repository, and the git CLI does not allow that, we can use either [GitHub's CLI](https://cli.github.com/manual/) `gh` (requires authentication) or Apache's subversion `svn` [[1]](https://cheatography.com/davechild/cheat-sheets/subversion/) [[2]](https://stackoverflow.com/questions/7106012/download-a-single-folder-or-directory-from-a-github-repo):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXNWPr17eBw"
      },
      "source": [
        "%%capture\n",
        "!sudo apt install subversion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ske6socCr6s"
      },
      "source": [
        "!svn checkout https://github.com/ImagingDataCommons/IDC-Examples/trunk/notebooks/nsclc-radiomics/demo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l1gGcYvwDZ5"
      },
      "source": [
        "!pip3 install -r demo/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xPYUyki_2Iz"
      },
      "source": [
        "For image pre-processing we will use [Plastimatch](https://plastimatch.org), a reliable and open source software for image computation.\n",
        "\n",
        "Plastimatch is available as an extension (plug-in) for 3D Slicer, but can also be used from the command line/from python scripts (using libraries such as `subprocess`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmBdfpob_Hin"
      },
      "source": [
        "%%capture\n",
        "!sudo apt update\n",
        "!sudo apt install plastimatch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP3g27cZAzX7"
      },
      "source": [
        "Verify the installation process was successful by checking Plastimatch version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXkhORXp_xT_"
      },
      "source": [
        "!plastimatch --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKFJDlDT7eWD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K-DE0R8rkO4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import sklearn \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import SimpleITK as sitk\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "print(\"Python version               : \", sys.version.split('\\n')[0])\n",
        "print(\"Numpy version                : \", np.__version__)\n",
        "print(\"TensorFlow version           : \", tf.__version__)\n",
        "print(\"Keras (stand-alone) version  : \", keras.__version__)\n",
        "\n",
        "print(\"\\nThis Colab instance is equipped with a GPU.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "#everything that has to do with plotting goes here below\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "## ----------------------------------------\n",
        "\n",
        "# create new colormap appending the alpha channel to the selected one\n",
        "# (so that we don't get a \\\"color overlay\\\" when plotting the segmask superimposed to the CT)\n",
        "cmap = plt.cm.Reds\n",
        "my_reds = cmap(np.arange(cmap.N))\n",
        "my_reds[:,-1] = np.linspace(0, 1, cmap.N)\n",
        "my_reds = ListedColormap(my_reds)\n",
        "\n",
        "cmap = plt.cm.jet\n",
        "my_jet = cmap(np.arange(cmap.N))\n",
        "my_jet[:,-1] = np.linspace(0, 1, cmap.N)\n",
        "my_jet = ListedColormap(my_jet)\n",
        "\n",
        "## ----------------------------------------\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzZAdA4Ku39L"
      },
      "source": [
        "---\n",
        "\n",
        "## Data Download\n",
        "\n",
        "The Imaging Data Commons GCS buckets are \"[requester pays](https://cloud.google.com/storage/docs/requester-pays)\" buckets. Hence, it is not possible to [mount such buckets directly in Colab](https://gist.github.com/korakot/f3600576720206363c734eca5f302e38).\n",
        "\n",
        "Instead, what the user can do is to query the BigQuery table associated to the bucket/dataset, select the cohort of interest, and then download the files exploiting `gsutil`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfj6eDV55TT8"
      },
      "source": [
        "<font size=\"5\" color=\"orange\"><b>This step (auth) won't be needed once the IDC bucket goes live</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZXwYFoEuRM6"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNJhhH9rH00w"
      },
      "source": [
        "---\n",
        "#### Exploiting BigQuery to Select which Data to Download\n",
        "\n",
        "Let's run a BigQuery query, exploiting the `%%bigquery` [IPython Magic](https://googleapis.dev/python/bigquery/latest/magics.html), to parse information regarding the dataset of interest (e.g., which subjects to download, the mapping between DICOM CTs and DICOM RTSTRUCTs/RTSEGs, ...).\n",
        "\n",
        "Using the following syntax, the result will be stored as a DataFrame in `cohort_df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INdlTxRgIJSS"
      },
      "source": [
        "%%bigquery --project=idc-sandbox-000 cohort_df\n",
        "\n",
        "WITH\n",
        "  ct_series AS (\n",
        "  SELECT\n",
        "    DISTINCT(PatientID),\n",
        "    StudyInstanceUID AS ctStudyInstanceUID,\n",
        "    SeriesInstanceUID AS ctSeriesInstanceUID\n",
        "  FROM\n",
        "    `idc-dev-etl.idc_tcia.idc_tcia`\n",
        "  WHERE\n",
        "    PatientID LIKE \"LUNG1%\"\n",
        "    AND Modality = \"CT\"\n",
        "  ORDER BY\n",
        "    PatientID),\n",
        "  rtstruct_series AS (\n",
        "  SELECT\n",
        "    DISTINCT(PatientID),\n",
        "    StudyInstanceUID AS rtstructStudyInstanceUID,\n",
        "    SeriesInstanceUID AS rtstructSeriesInstanceUID\n",
        "  FROM\n",
        "    `idc-dev-etl.idc_tcia.idc_tcia`\n",
        "  WHERE\n",
        "    PatientID LIKE \"LUNG1%\"\n",
        "    AND Modality = \"RTSTRUCT\"\n",
        "  ORDER BY\n",
        "    PatientID),\n",
        "  seg_series AS (\n",
        "  SELECT\n",
        "    DISTINCT(PatientID),\n",
        "    StudyInstanceUID AS segStudyInstanceUID,\n",
        "    SeriesInstanceUID AS segSeriesInstanceUID\n",
        "  FROM\n",
        "    `idc-dev-etl.idc_tcia.idc_tcia`\n",
        "  WHERE\n",
        "    PatientID LIKE \"LUNG1%\"\n",
        "    AND Modality = \"SEG\"\n",
        "  ORDER BY\n",
        "    PatientID)\n",
        "SELECT\n",
        "  PatientID,\n",
        "  ctStudyInstanceUID,\n",
        "  ctSeriesInstanceUID,\n",
        "  rtstructStudyInstanceUID,\n",
        "  rtstructSeriesInstanceUID,\n",
        "  segStudyInstanceUID,\n",
        "  segSeriesInstanceUID\n",
        "FROM\n",
        "  ct_series\n",
        "JOIN\n",
        "  rtstruct_series\n",
        "using (PatientID)\n",
        "JOIN\n",
        "  seg_series\n",
        "USING\n",
        "  (PatientID)\n",
        "ORDER BY\n",
        "  PatientID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxy9Dn08IJcx"
      },
      "source": [
        "cohort_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o77XEJFXOVXm"
      },
      "source": [
        "After selecting a few subjects from the cohort, exploiting the output of the BigQuery query, populate a dictionary with the `ctStudyInstanceUID` and `rtstructStudyInstanceUID` values. Finally, exploit `gsutil` to download such sub-cohort:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4UwD0oHOfxq"
      },
      "source": [
        "n_patients = 10\n",
        "\n",
        "# useful for logging purposes\n",
        "download_dict = dict()\n",
        "\n",
        "# since gsutil can be called with \"-m\" providing a list as an input,\n",
        "# append each gs URI to download to a list:\n",
        "to_download = list()\n",
        "base_gs_uri = 'gs://idc-tcia-1-nsclc-radiomics/dicom/'\n",
        "\n",
        "# analysis baseline: Hosny et Al. results\n",
        "baseline_csv_name = 'nsclc-radiomics_hosny_baseline.csv'\n",
        "baseline_csv_path = os.path.join('demo', baseline_csv_name)\n",
        "baseline_df = pd.read_csv(baseline_csv_path)\n",
        "\n",
        "# make sure the selected sub-cohort was analysed in full by Hosny et Al. \n",
        "#subcohort_df = cohort_df.sample(n = n_patients)\n",
        "\n",
        "# list of the NSCLC-Radiomics subjects analysed in Hosny et Al.\n",
        "baseline_subj_list = [a[1:] for a in list(baseline_df[\"id\"].dropna())]\n",
        "\n",
        "# intersection between the two sets\n",
        "common_subj_list = list(set(baseline_subj_list) & set(cohort_df[\"PatientID\"]))\n",
        "\n",
        "assert len(baseline_subj_list) == len(common_subj_list)\n",
        "\n",
        "# populate a dataset with n_subjects sampled from this pool only\n",
        "subcohort_df = cohort_df[cohort_df[\"PatientID\"].isin(common_subj_list)].sample(n = n_patients)\n",
        "\n",
        "\n",
        "for pat_num, pat in enumerate(list(subcohort_df[\"PatientID\"])):\n",
        "  print(\"(%g/%g) - PatientID: %s\"%(pat_num + 1, n_patients, pat), end = '\\r')\n",
        "\n",
        "  pat_df = subcohort_df[subcohort_df[\"PatientID\"] == pat]\n",
        "  download_dict[pat] = dict()\n",
        "  download_dict[pat][\"ctStudyInstanceUID\"] = pat_df[\"ctStudyInstanceUID\"].values[0]\n",
        "  download_dict[pat][\"rtstructStudyInstanceUID\"] = pat_df[\"rtstructStudyInstanceUID\"].values[0]\n",
        "\n",
        "  to_download.append(base_gs_uri + download_dict[pat][\"ctStudyInstanceUID\"])\n",
        "  to_download.append(base_gs_uri + download_dict[pat][\"rtstructStudyInstanceUID\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkrR0e1KF8Hs"
      },
      "source": [
        "# populate a dataframe with such gs URI (again, mostly for logging purposes)\n",
        "manifesto_dict = {\"gs_uri\" : to_download}\n",
        "manifesto_df = pd.DataFrame(manifesto_dict, columns = [\"gs_uri\"])\n",
        "\n",
        "manifesto_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ32gPLqFyd1"
      },
      "source": [
        "# generate a text file, which will be parsed by gsutil to download the selected patients:\n",
        "manifesto_df.to_csv(\"gcs_paths.txt\", header = False, index = False)\n",
        "\n",
        "# check everything went as expected\n",
        "!head /content/gcs_paths.txt -n 2\n",
        "!echo \"...\" && echo \"\"\n",
        "\n",
        "# the number of lines in the file should be equal to twice the specified \"n_patients\"\n",
        "!echo \"Number of lines in the file:\" $(cat /content/gcs_paths.txt | wc -l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E4hNYNDcvUG"
      },
      "source": [
        "---\n",
        "\n",
        "#### DICOM Data Download\n",
        "\n",
        "The following instructions will download the DICOM CT and DICOM RTSTRUCT files for `n_patients` patients. For `n_patients = 10`, the download should take approximately 3-4 minutes (roughly 1-1.5k files, a total of 500-700MB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_N47fkDnVPp"
      },
      "source": [
        "# if everything is allright, proceed with the download\n",
        "!mkdir -p data/nsclc-radiomics/dicom\n",
        "\n",
        "!cat gcs_paths.txt | gsutil -u idc-sandbox-000 -m cp -Ir ./data/nsclc-radiomics/dicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny5UgQzjnTxU"
      },
      "source": [
        "---\n",
        "\n",
        "Check the disk space after the download:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JrgIb1d471w"
      },
      "source": [
        "!df -h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbc2RVQp52C9"
      },
      "source": [
        "!du -h ./data/nsclc-radiomics -d 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUrvOf3GZzh8"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Data Pre-processing\n",
        "\n",
        "Let us explot the python scripts under `demo` to preprocess the data:\n",
        "\n",
        "* First of all, each DICOM Series (CT and RTSTRUCT) is converted from DICOM to NRRD using plastimatch. Following the authors pipeline, we then resample all the volumes to 1-mm isotropic:\n",
        "  * In this case, the interpolation step uses a linear strategy but, as we will investigate later, the model is robust to the textural differences introduced by a different interpolation strategy, such as a nearest neighbour one);\n",
        "  * A .png image is exported for quality control, together with other potentially useful information, and can be found in the patient folder (under `nsclc-radiomics_preprocessed`);\n",
        "* After the conversion and the resampling, other application-specific transformations are applied to the data. The main tumour Center of Mass (CoM) is computed starting from the labelled GTV, and a $150 \\times 150 \\times 150$ subvolume is cropped around such coordinate.\n",
        "\n",
        "<br>\n",
        "\n",
        "In order to save space on the Colab instance partition, only the final $150 \\times 150 \\times 150$ subvolumes (both CT and the GTV segmentation mask, saved in NRRD) are kept. \n",
        "\n",
        "All the informations regarding the crop, the list of segmentation masks found in the RTSTRUCT, and the CoM - together with the aforementioned quality-control pngs, are saved under the patient folder in the following fashion:\n",
        "\n",
        "```\n",
        "data\n",
        "    |_nsclc-radiomics_preprocessed\n",
        "                                  |_nrrd\n",
        "                                        |_LUNG1-XYZ\n",
        "                                                   |_LUNG1-XYZ_whole_ct_rt\n",
        "                                                   |_LUNG1-XYZ_com_log.json\n",
        "                                                   |_LUNG1-XYZ_lookup_info.json\n",
        "                                                   ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1d3SZq0Ny3P"
      },
      "source": [
        "from demo.data_utils import *\n",
        "from demo.utils import *\n",
        "\n",
        "# FIXME: DEBUG\n",
        "\n",
        "#import importlib\n",
        "#importlib.reload(data_utils)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giRh3bneYt7e"
      },
      "source": [
        "data_base_path = 'data'\n",
        "\n",
        "dataset_name = 'nsclc-radiomics'\n",
        "dataset_path = os.path.join(data_base_path, dataset_name)\n",
        "data_path = os.path.join(dataset_path, 'dicom')\n",
        "\n",
        "preproc_dataset_name = dataset_name + '_preprocessed'\n",
        "preproc_dataset_path = os.path.join(data_base_path, preproc_dataset_name)\n",
        "preproc_data_path = os.path.join(preproc_dataset_path, 'nrrd')\n",
        "    \n",
        "if not os.path.exists(preproc_data_path):\n",
        "    os.makedirs(preproc_data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdOOu_5wNzBq"
      },
      "source": [
        "for pat_num, pat in enumerate(subcohort_df.PatientID.values):\n",
        "\n",
        "    # clear cell output before moving to the next (goes at the top to clean what comes next)\n",
        "    clear_output(wait = True)\n",
        "    \n",
        "    print(\"\\nPatient %d/%d (%s)\"%(pat_num + 1, len(subcohort_df), pat))\n",
        "    \n",
        "    pat_dir_path = os.path.join(preproc_data_path, pat)\n",
        "    \n",
        "    if not os.path.exists(pat_dir_path):\n",
        "        os.mkdir(pat_dir_path)\n",
        "    \n",
        "    # location where the tmp nrrd files (resampled CT/RTSTRUCT nrrd) should be saved\n",
        "    # by the \"export_res_nrrd_from_dicom\" function found in preprocess.py\n",
        "    ct_nrrd_path = os.path.join(pat_dir_path, pat + '_ct_resampled.nrrd')\n",
        "    rt_nrrd_path = os.path.join(pat_dir_path, pat + '_rt_resampled.nrrd')\n",
        "\n",
        "    # location where the nrrd files (cropped resampled CT/RTSTRUCT nrrd) should be saved\n",
        "    # by the \"export_com_subvolume\" function found in preprocess.py\n",
        "    ct_nrrd_crop_path = os.path.join(pat_dir_path, pat + '_ct_res_crop.nrrd')\n",
        "    rt_nrrd_crop_path = os.path.join(pat_dir_path, pat + '_rt_res_crop.nrrd')\n",
        "    \n",
        "    # if the latter are already there, skip the processing\n",
        "    if os.path.exists(ct_nrrd_crop_path) and os.path.exists(rt_nrrd_crop_path):\n",
        "        print(\"%s\\nand\\n%s\\nfound, skipping the processing for patient %s...\"%(ct_nrrd_crop_path,\n",
        "                                                                               rt_nrrd_crop_path, \n",
        "                                                                               pat))\n",
        "        continue\n",
        "    \n",
        "    ## ----------------------------------------\n",
        "    \n",
        "    pat_df = subcohort_df[subcohort_df[\"PatientID\"] == pat]\n",
        "\n",
        "\n",
        "    path_to_ct_dir = os.path.join(data_path,\n",
        "                                  pat_df[\"ctStudyInstanceUID\"].values[0],\n",
        "                                  pat_df[\"ctSeriesInstanceUID\"].values[0])\n",
        "\n",
        "    path_to_rt_dir = os.path.join(data_path,\n",
        "                                  pat_df[\"rtstructStudyInstanceUID\"].values[0],\n",
        "                                  pat_df[\"rtstructSeriesInstanceUID\"].values[0])\n",
        "\n",
        "    path_to_seg_dir = os.path.join(data_path, \n",
        "                                   pat_df[\"segStudyInstanceUID\"].values[0], \n",
        "                                   pat_df[\"segSeriesInstanceUID\"].values[0])\n",
        "\n",
        "    # FIXME: sanity check\n",
        "    assert os.path.exists(path_to_ct_dir)\n",
        "    assert os.path.exists(path_to_rt_dir)\n",
        "    assert os.path.exists(path_to_seg_dir)    \n",
        "    \n",
        "    # log lookup informations (human-readable to StudyUID and SeriesUID)\n",
        "    lookup_dict_path = os.path.join(pat_dir_path, pat + '_lookup_info.json')\n",
        "    \n",
        "    lookup_dict = dict()\n",
        "    lookup_dict[pat] = dict()\n",
        "    \n",
        "    lookup_dict[pat][\"path_to_ct_dir\"] = path_to_ct_dir\n",
        "    lookup_dict[pat][\"ctStudyInstanceUID\"] = pat_df[\"ctStudyInstanceUID\"].values[0]\n",
        "    lookup_dict[pat][\"ctSeriesInstanceUID\"] = pat_df[\"ctSeriesInstanceUID\"].values[0]\n",
        "    \n",
        "    lookup_dict[pat][\"path_to_rt_dir\"] = path_to_rt_dir\n",
        "    lookup_dict[pat][\"rtstructStudyInstanceUID\"] = pat_df[\"rtstructStudyInstanceUID\"].values[0]\n",
        "    lookup_dict[pat][\"rtstructSeriesInstanceUID\"] = pat_df[\"rtstructSeriesInstanceUID\"].values[0]\n",
        "    \n",
        "    # FIXME: not used so far but still, useful to log for future purposes\n",
        "    lookup_dict[pat][\"path_to_seg_dir\"] = path_to_seg_dir\n",
        "    lookup_dict[pat][\"segStudyInstanceUID\"] = pat_df[\"segStudyInstanceUID\"].values[0]\n",
        "    lookup_dict[pat][\"segSeriesInstanceUID\"] = pat_df[\"segSeriesInstanceUID\"].values[0]\n",
        "    \n",
        "    with open(lookup_dict_path, 'w') as json_file:\n",
        "        json.dump(lookup_dict, json_file, indent = 2)\n",
        "    \n",
        "    ## ----------------------------------------\n",
        "\n",
        "    proc_log = export_res_nrrd_from_dicom(dicom_ct_path = path_to_ct_dir, \n",
        "                                          dicom_rt_path = path_to_rt_dir, \n",
        "                                          output_dir = pat_dir_path, \n",
        "                                          pat_id = pat,\n",
        "                                          output_dtype = \"float\")\n",
        "    \n",
        "    # check every step of the DICOM to NRRD conversion returned 0 (everything's ok)\n",
        "    assert(np.sum(np.array(list(proc_log.values()))) == 0)\n",
        "    \n",
        "    # FIXME: sanity check\n",
        "    assert(os.path.exists(ct_nrrd_path))\n",
        "    assert(os.path.exists(rt_nrrd_path))\n",
        "    \n",
        "    sitk_vol = sitk.ReadImage(ct_nrrd_path)\n",
        "    vol = sitk.GetArrayFromImage(sitk_vol)\n",
        "    \n",
        "    sitk_seg = sitk.ReadImage(rt_nrrd_path)\n",
        "    seg = sitk.GetArrayFromImage(sitk_seg)\n",
        "    \n",
        "    # FIXME: sanity check\n",
        "    assert(vol.shape == seg.shape)\n",
        "    \n",
        "    com = compute_center_of_mass(seg)\n",
        "    com_int = [int(coord) for coord in com]\n",
        "\n",
        "    # export the CoM slice (CT + RTSTRUCT) for quality control\n",
        "    export_png_slice(input_volume = vol,\n",
        "                     input_segmask = seg,\n",
        "                     fig_out_path = os.path.join(pat_dir_path, pat + '_whole_CT_CoM.png'),\n",
        "                     fig_dpi = 220,\n",
        "                     lon_slice_idx = com_int[0],\n",
        "                     cor_slice_idx = com_int[1],\n",
        "                     sag_slice_idx = com_int[2],\n",
        "                     z_first = True)\n",
        "    \n",
        "    # crop a (150, 150, 150) subvolume from the resampled scans, get rid of the latter\n",
        "    proc_log = export_com_subvolume(ct_nrrd_path = ct_nrrd_path, \n",
        "                                    rt_nrrd_path = rt_nrrd_path, \n",
        "                                    crop_size = (150, 150, 150), \n",
        "                                    output_dir = pat_dir_path,\n",
        "                                    pat_id = pat,\n",
        "                                    z_first = True, \n",
        "                                    rm_orig = True)\n",
        "    \n",
        "    # log CoM information\n",
        "    com_log_path = os.path.join(pat_dir_path, pat + '_com_log.json')\n",
        "    com_log_dict = {k : v for (k, v) in proc_log.items() if \"com_int\" in k}\n",
        "    \n",
        "    with open(com_log_path, 'w') as json_file:\n",
        "        json.dump(com_log_dict, json_file, indent = 2)\n",
        "    \n",
        "    # if CoM calculation goes wrong then continue\n",
        "    proc_log_crop = {k : v for (k, v) in proc_log.items() if \"cropping\" in k}\n",
        "    if len(proc_log_crop) == 0:\n",
        "        os.remove(ct_nrrd_path)\n",
        "        os.remove(rt_nrrd_path)\n",
        "        continue\n",
        "    \n",
        "    # check the cropped volumes have been exported as intended\n",
        "    assert(np.sum(np.array(list(proc_log_crop.values()))) == 0)\n",
        "    assert(os.path.exists(ct_nrrd_crop_path))\n",
        "    assert(os.path.exists(rt_nrrd_crop_path))\n",
        "    \n",
        "    sitk_vol = sitk.ReadImage(ct_nrrd_crop_path)\n",
        "    vol_crop = sitk.GetArrayFromImage(sitk_vol)\n",
        "\n",
        "    sitk_seg = sitk.ReadImage(rt_nrrd_crop_path)\n",
        "    seg_crop = sitk.GetArrayFromImage(sitk_seg)\n",
        "    \n",
        "    # export the cropped subvolume CoM slice (CT + RTSTRUCT) for quality control\n",
        "    export_png_slice(input_volume = vol_crop,\n",
        "                     input_segmask = seg_crop,\n",
        "                     fig_out_path = os.path.join(pat_dir_path, pat + '_crop_CT_CoM.png'),\n",
        "                     fig_dpi = 220,\n",
        "                     lon_slice_idx = 75,\n",
        "                     cor_slice_idx = 75,\n",
        "                     sag_slice_idx = 75,\n",
        "                     z_first = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7zGjKKZn3tX"
      },
      "source": [
        "---\n",
        "\n",
        "### Logging All the Processing Details\n",
        "\n",
        "In order to make sure the whole pipeline is easily reproducible, let's log all the details in `\u001cnsclc-radiomics_preprocessed/nscls-radiomics_preproc_details.csv`.\n",
        "\n",
        "The CSV file contains information regarding:\n",
        "* The patient ID;\n",
        "* The relative paths to the folders containing DICOM CT, DICOM RTSTRUCT and DICOM RTSEG Series;\n",
        "* The Study and Series Instance UID of all the Series;\n",
        "* The shape of the original DICOM Series and the shape of the resampled to 1mm isotropic NRRD volumes;\n",
        "* The name of the label used to compute the CoM (as stored in the DICOM RTSTRUCT Series, and thus exported by Plastimatch), the CoM (integer coordinates), the bounding box size and its coordinates in the resampled volume space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5o6R9j3n3Ls"
      },
      "source": [
        "csv_out_name = 'nsclc-radiomics_preproc_details.csv'\n",
        "dataset_csv_path = os.path.join(data_base_path, csv_out_name)\n",
        "\n",
        "\n",
        "df_keys = ['PatientID',\n",
        "           'path_to_ct_dir', 'ctStudyInstanceUID', 'ctSeriesInstanceUID',\n",
        "           'path_to_rt_dir', 'rtstructStudyInstanceUID', 'rtstructSeriesInstanceUID',\n",
        "           'path_to_seg_dir', 'segStudyInstanceUID', 'segSeriesInstanceUID',\n",
        "           'rt_exported', 'orig_shape', '1mm_iso_shape', 'crop_shape', 'com_int', 'bbox']\n",
        "\n",
        "data = {k : list() for k in df_keys}\n",
        "\n",
        "det_df = pd.DataFrame(data = data, dtype = object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n20Kc99MqO0M"
      },
      "source": [
        "for pat_num, pat in enumerate(subcohort_df.PatientID.values):\n",
        "\n",
        "    print(\"\\rProcessing patient '%s' (%d/%d)... \"%(pat, pat_num + 1,\n",
        "                                                   len(subcohort_df.PatientID)),\n",
        "          end = '')\n",
        "    \n",
        "    pat_df = subcohort_df[subcohort_df['PatientID'] == pat]\n",
        "            \n",
        "    # init a dictionary with the same keys as \"df_keys\" to populate the latter\n",
        "    pat_dict = dict()\n",
        "    pat_dict[\"PatientID\"] = pat\n",
        "\n",
        "    pat_dir_path = os.path.join(preproc_data_path, pat)\n",
        "    pat_json_path = os.path.join(pat_dir_path, pat + '_lookup_info.json')\n",
        "\n",
        "    with open(pat_json_path, 'r') as json_file:\n",
        "        lookup_dict = json.load(json_file)\n",
        "    \n",
        "    pat_dict[\"path_to_ct_dir\"] = lookup_dict[pat][\"path_to_ct_dir\"]\n",
        "    pat_dict[\"ctStudyInstanceUID\"] = pat_df['ctStudyInstanceUID'].values[0]\n",
        "    pat_dict[\"ctSeriesInstanceUID\"] = pat_df['ctSeriesInstanceUID'].values[0]\n",
        "\n",
        "    pat_dict[\"path_to_rt_dir\"] = lookup_dict[pat][\"path_to_rt_dir\"]\n",
        "    pat_dict[\"rtstructStudyInstanceUID\"] = pat_df['rtstructStudyInstanceUID'].values[0]\n",
        "    pat_dict[\"rtstructSeriesInstanceUID\"] = pat_df['rtstructSeriesInstanceUID'].values[0]\n",
        "\n",
        "    pat_dict[\"path_to_seg_dir\"] = lookup_dict[pat][\"path_to_seg_dir\"]\n",
        "    pat_dict[\"segStudyInstanceUID\"] = pat_df['segStudyInstanceUID'].values[0]\n",
        "    pat_dict[\"segSeriesInstanceUID\"] = pat_df['segSeriesInstanceUID'].values[0]\n",
        "    \n",
        "    # ----------------------------------------\n",
        "    \n",
        "    # populate the \"rt_exported\" field\n",
        "    rt_folder = os.path.join(pat_dir_path, pat  + '_whole_ct_rt')\n",
        "    pat_dict['rt_exported'] = [f for f in os.listdir(rt_folder) if 'gtv-1' in f.lower()][0].split('.nrrd')[0]   \n",
        "    if pat_dict['rt_exported'] != 'GTV-1':\n",
        "        a.append(pat)\n",
        "    \n",
        "    # ----------------------------------------\n",
        "    \n",
        "    dicom_ct_path = lookup_dict[pat][\"path_to_ct_dir\"]\n",
        "    \n",
        "    dcm_file_path = os.path.join(dicom_ct_path,                 # parent folder\n",
        "                                 os.listdir(dicom_ct_path)[0])  # *.dcm files\n",
        "\n",
        "    dcm_file = pydicom.dcmread(dcm_file_path)\n",
        "    n_dcm_files = len([f for f in os.listdir(dicom_ct_path) if '.dcm' in f])\n",
        "\n",
        "    xy = int(float(dcm_file.Rows)*float(dcm_file.PixelSpacing[0]))\n",
        "    z = int(float(dcm_file.SliceThickness)*float(n_dcm_files))\n",
        "    \n",
        "    orig_dcm_shape = (n_dcm_files, dcm_file.Columns, dcm_file.Rows)\n",
        "    res_dcm_shape = (z, xy, xy)\n",
        "    \n",
        "    pat_dict['orig_shape'] = orig_dcm_shape\n",
        "    pat_dict['1mm_iso_shape'] = res_dcm_shape\n",
        "    \n",
        "    # ----------------------------------------\n",
        "     \n",
        "    com_json_path = os.path.join(pat_dir_path, pat + '_com_log.json')\n",
        "    \n",
        "    try:\n",
        "        with open(com_json_path, 'r') as json_file:\n",
        "            com_dict = json.load(json_file)\n",
        "            pat_dict['com_int'] = tuple(com_dict[\"com_int\"])\n",
        "    except:\n",
        "        print('_com_log.json loading error;')\n",
        "    \n",
        "    # ----------------------------------------\n",
        "    \n",
        "    bbox_json_path = os.path.join(pat_dir_path, pat + '_crop_log.json')\n",
        "    \n",
        "    try:\n",
        "        with open(bbox_json_path, 'r') as json_file:\n",
        "            bbox_dict = json.load(json_file)\n",
        "            pat_dict['bbox'] = bbox_dict\n",
        "    except:\n",
        "        print('_crop_log.json loading error;')\n",
        "    \n",
        "    # ----------------------------------------\n",
        "    ct_res_crop_path = os.path.join(pat_dir_path, pat + '_ct_res_crop.nrrd')\n",
        "    \n",
        "    try:\n",
        "        sitk_ct_res_crop = sitk.ReadImage(ct_res_crop_path)\n",
        "        pat_dict['crop_shape'] = sitk_ct_res_crop.GetSize()\n",
        "    except:\n",
        "        print('_ct_res_crop.nrrd loading error;')\n",
        "        \n",
        "    # ----------------------------------------\n",
        "    \n",
        "    det_df = det_df.append(pat_dict, ignore_index = True)\n",
        "\n",
        "det_df.to_csv(dataset_csv_path, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-kUOASHmvMJ"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# <center> Data Exploration </center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqXbqLUj2l4J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH98IdJY1fDh"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# <center> Data Processing </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRl0JkQJ2L5X"
      },
      "source": [
        "Brief description of the model here (image from the paper, or description of the architecture, also from the paper)?\n",
        "\n",
        "Then, load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5pNdkZw1hKP"
      },
      "source": [
        "arch_json_path = \"demo/architecture.json\"\n",
        "weights_path = \"demo/weights.h5\"\n",
        "\n",
        "\"\"\"\n",
        " N.B. the warnings are due to the fact that the model was developed for\n",
        " Keras 1, and the config file has been converted in a Keras-2-compatible file\n",
        "\n",
        " nonetheless, Keras 2 uses different naming conventions/def.s, so in order to\n",
        " get rid of the warnings one should change all the layers def.s in the JSON file\n",
        "\"\"\"\n",
        "\n",
        "# load the model architecture from the config file, then load the model weights \n",
        "with open(arch_json_path, 'r') as json_file:\n",
        "    model_json = json.load(json_file)  \n",
        "\n",
        "model = keras.models.model_from_config(model_json)\n",
        "model.summary()\n",
        "\n",
        "model.load_weights(weights_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RUyCsnm2-BV"
      },
      "source": [
        "\"\"\"\n",
        "# sanity check on model weights + visualisation?\n",
        "assert np.sum(model.get_weights()[1]) != 0\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  #This way\n",
        "  tf.global_variables_initializer().run()\n",
        "  \n",
        "  bn1_weights = model.layers[1].weights[2].value()\n",
        "  \n",
        "  print(model.layers[1].weights[2].name)\n",
        "  print(\"weights:\", sess.run(bn1_weights))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wQ1B4R9l2Hl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8oKJlrom9vx"
      },
      "source": [
        "# define a new dataframe to store basics information + baseline output\n",
        "# as well as the reproduced experiment output\n",
        "df_keys = ['PatientID', 'StudyInstanceUID', 'SeriesInstanceUID_CT',\n",
        "           'SeriesInstanceUID_RTSTRUCT', 'CNN_output_raw', 'CNN_output_argmax',\n",
        "           'baseline_output_raw', 'baseline_output_argmax', 'surv2yr'\n",
        "           ]\n",
        "\n",
        "data = {k : list() for k in df_keys}\n",
        "\n",
        "out_df = pd.DataFrame(data, dtype = object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SNCL2ZOl29L"
      },
      "source": [
        "y_pred_dict = dict()\n",
        "\n",
        "input_df = pd.read_csv(dataset_csv_path) \n",
        "input_subj_list = list(input_df[\"PatientID\"])\n",
        "\n",
        "\"\"\"\n",
        "# analysis baseline: Hosny et Al. results\n",
        "baseline_csv_name = 'nsclc-radiomics_hosny_baseline.csv'\n",
        "baseline_csv_path = os.path.join('demo', baseline_csv_name)\n",
        "baseline_df = pd.read_csv(baseline_csv_path)\n",
        "\"\"\"\n",
        "\n",
        "for idx, subj in enumerate(input_subj_list):\n",
        "\n",
        "    print(\"Processing subject '%s' (%d/%d)... \"%(subj, idx + 1, len(input_subj_list)), end = '\\r')\n",
        "\n",
        "    \"\"\"\n",
        "    The NRRD files for each subject in \"input_df\" should exist and readable\n",
        "    (already double checked during the creation of 'lung1_proc_details.csv').\n",
        "    If not, just run the code in  'lung1_det_csv.ipynb', found under /src.\n",
        "    \"\"\"\n",
        "    \n",
        "    subj_df = input_df[input_df['PatientID'] == subj]\n",
        "    \n",
        "    ct_res_crop_path = os.path.join(preproc_data_path, subj, subj + '_ct_res_crop.nrrd')\n",
        "    \n",
        "    input_vol = get_input_volume(input_ct_nrrd_path = ct_res_crop_path)\n",
        "    input_vol = np.expand_dims(input_vol, axis = 0)\n",
        "    input_vol = np.expand_dims(input_vol, axis = -1)\n",
        "    \n",
        "    y_pred_raw = model.predict(input_vol)\n",
        "    y_pred_argmax = int(np.argmax(y_pred_raw[0]))\n",
        "    \n",
        "    subj_dict = dict()\n",
        "    subj_dict[\"PatientID\"] = subj\n",
        "    \n",
        "    subj_dict[\"StudyInstanceUID\"] = subj_df[\"ctStudyInstanceUID\"].values[0]\n",
        "    subj_dict[\"SeriesInstanceUID_CT\"] = subj_df[\"ctSeriesInstanceUID\"].values[0]\n",
        "    subj_dict[\"SeriesInstanceUID_RTSTRUCT\"] = subj_df[\"rtstructSeriesInstanceUID\"].values[0]\n",
        "\n",
        "    subj_dict[\"CNN_output_raw\"] = y_pred_raw.tolist()[0]\n",
        "    subj_dict[\"CNN_output_argmax\"] = y_pred_argmax\n",
        "    \n",
        "    baseline_output_list = list()\n",
        "    \n",
        "    try:\n",
        "        baseline_output_list.append(baseline_df[baseline_df[\"id\"] == ' %s'%(subj)][\"logit_0\"].values[0])\n",
        "        baseline_output_list.append(baseline_df[baseline_df[\"id\"] == ' %s'%(subj)][\"logit_1\"].values[0])\n",
        "\n",
        "        subj_dict['baseline_output_raw'] = np.array(baseline_output_list)\n",
        "        subj_dict['baseline_output_argmax'] = int(np.argmax(np.array(baseline_output_list)))\n",
        "        \n",
        "        subj_dict['surv2yr'] = baseline_df[baseline_df[\"id\"] == ' %s'%(subj)][\"surv2yr\"].values[0]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    out_df = out_df.append(subj_dict, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssOdPW4inFrw"
      },
      "source": [
        "out_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luyu1ps7s7Vc"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# <center> Visualising the Results </center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOzIwhoks_BQ"
      },
      "source": [
        "# replication\n",
        "y_true = np.stack(out_df[\"surv2yr\"].values)\n",
        "y_pred = np.stack(out_df[\"CNN_output_raw\"].values)\n",
        "\n",
        "fpr, tpr, thr_roc = sklearn.metrics.roc_curve(y_true, y_pred[:, 1])\n",
        "prc, rec, thr_pr = sklearn.metrics.precision_recall_curve(y_true, y_pred[:, 1])\n",
        "\n",
        "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
        "pr_auc = sklearn.metrics.auc(rec, prc)\n",
        "\n",
        "print(\"ROC AUC: %g\"%(roc_auc))\n",
        "print(\"PR AUC: %g\"%(pr_auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StmKbN5ovwtL"
      },
      "source": [
        "# Hosny et Al. model\n",
        "y_pred_baseline = np.stack(out_df[\"baseline_output_raw\"].values)\n",
        "\n",
        "fpr_base, tpr_base, thr_roc_base = sklearn.metrics.roc_curve(y_true, \n",
        "                                                             y_pred_baseline[:, 1])\n",
        "\n",
        "prc_base, rec_base, thr_pr_base = sklearn.metrics.precision_recall_curve(y_true,\n",
        "                                                                         y_pred_baseline[:, 1])\n",
        "\n",
        "roc_auc_baseline = sklearn.metrics.auc(fpr_base, tpr_base)\n",
        "pr_auc_baseline = sklearn.metrics.auc(rec_base, prc_base)\n",
        "\n",
        "print(\"ROC AUC: %g\"%(roc_auc_baseline))\n",
        "print(\"PR AUC: %g\"%(pr_auc_baseline))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHuvzII9wSi-"
      },
      "source": [
        "# operating point\n",
        "opp = 0.5\n",
        "\n",
        "opp_roc = np.argmin(np.abs(thr_roc - opp))\n",
        "opp_pr = np.argmin(np.abs(thr_pr - opp))\n",
        "\n",
        "print('ROC OPP: FPR = %2.4f, TPR = %2.4f'%(fpr[opp_roc], tpr[opp_roc]))\n",
        "print('PR OPP: PRC = %2.4f, REC = %2.4f'%(prc[opp_pr], rec[opp_pr]))\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "opp_baseline = 0.5\n",
        "\n",
        "opp_roc_baseline = np.argmin(np.abs(thr_roc_base - opp_baseline))\n",
        "opp_pr_baseline = np.argmin(np.abs(thr_pr_base - opp_baseline))\n",
        "\n",
        "print('\\nROC OPP: FPR = %2.4f, TPR = %2.4f'%(fpr_base[opp_roc_baseline], tpr_base[opp_roc_baseline]))\n",
        "print('PR OPP: PRC = %2.4f, REC = %2.4f'%(prc_base[opp_pr_baseline], rec_base[opp_pr_baseline]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFbaTGghxiVT"
      },
      "source": [
        "sns.set()\n",
        "\n",
        "fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (12, 12))\n",
        "\n",
        "# plot ROC curve\n",
        "ax0.plot(fpr, tpr, label = 'ROC AUC: %2.2f'%(roc_auc))\n",
        "\n",
        "# plot operating point on ROC curve\n",
        "ax0.plot(fpr[opp_roc], tpr[opp_roc], label = 'CNN Operating Point', marker = '^', color = 'red')\n",
        "ax0.legend(loc = 'lower right')\n",
        "ax0.set_aspect('equal', 'box')\n",
        "ax0.set_xlim([-0.01, 1.01])\n",
        "ax0.set_ylim([-0.01, 1.01])\n",
        "ax0.set_xlabel('FPR')\n",
        "ax0.set_ylabel('TPR')\n",
        "ax0.set_title('ROC curve - replicated pre-processing pipeline')\n",
        "\n",
        "## ----------------------------------------\n",
        "\n",
        "# plot PR curve\n",
        "ax1.plot(rec, prc, label = 'PR AUC: %2.2f'%(pr_auc))\n",
        "\n",
        "# plot operating point on PR curve\n",
        "ax1.plot(rec[opp_pr], prc[opp_pr], label = 'CNN Operating Point', marker = '^', color = 'red')\n",
        "ax1.legend(loc = 'upper right')\n",
        "ax1.set_aspect('equal', 'box')\n",
        "ax1.set_xlim([-0.01, 1.01])\n",
        "ax1.set_ylim([-0.01, 1.01])\n",
        "ax1.set_xlabel('Recall')\n",
        "ax1.set_ylabel('Precision')\n",
        "ax1.set_title('PR curve - replicated pre-processing pipeline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4UCG7tFyGYn"
      },
      "source": [
        "sns.set()\n",
        "\n",
        "fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (12, 12))\n",
        "\n",
        "# plot ROC curve\n",
        "ax0.plot(fpr_base, tpr_base, label = 'ROC AUC: %2.2f'%(roc_auc_baseline))\n",
        "\n",
        "# plot operating point on ROC curve\n",
        "ax0.plot(fpr_base[opp_roc_baseline], tpr_base[opp_roc_baseline],\n",
        "         label = 'CNN Operating Point', marker = '^', color = 'red')\n",
        "ax0.legend(loc = 'lower right')\n",
        "ax0.set_aspect('equal', 'box')\n",
        "ax0.set_xlim([-0.01, 1.01])\n",
        "ax0.set_ylim([-0.01, 1.01])\n",
        "ax0.set_xlabel('FPR')\n",
        "ax0.set_ylabel('TPR')\n",
        "ax0.set_title('ROC curve - results from Hosny et Al.')\n",
        "\n",
        "## ----------------------------------------\n",
        "\n",
        "# plot PR curve\n",
        "ax1.plot(rec_baseline, prc_baseline, label = 'PR AUC: %2.2f'%(pr_auc_baseline))\n",
        "\n",
        "# plot operating point on PR curve\n",
        "ax1.plot(rec_baseline[opp_pr_baseline], prc_baseline[opp_pr_baseline],\n",
        "         label = 'CNN Operating Point', marker = '^', color = 'red')\n",
        "ax1.legend(loc = 'upper right')\n",
        "ax1.set_aspect('equal', 'box')\n",
        "ax1.set_xlim([-0.01, 1.01])\n",
        "ax1.set_ylim([-0.01, 1.01])\n",
        "ax1.set_xlabel('Recall')\n",
        "ax1.set_ylabel('Precision')\n",
        "ax1.set_title('PR curve - results from Hosny et Al.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSV5JuljyijI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}